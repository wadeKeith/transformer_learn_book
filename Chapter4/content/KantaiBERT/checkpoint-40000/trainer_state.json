{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 14.970059880239521,
  "eval_steps": 500,
  "global_step": 40000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.18712574850299402,
      "grad_norm": 3.0724408626556396,
      "learning_rate": 4.953218562874252e-05,
      "loss": 6.584,
      "step": 500
    },
    {
      "epoch": 0.37425149700598803,
      "grad_norm": 4.440441608428955,
      "learning_rate": 4.906437125748503e-05,
      "loss": 5.6518,
      "step": 1000
    },
    {
      "epoch": 0.561377245508982,
      "grad_norm": 4.794414520263672,
      "learning_rate": 4.859655688622755e-05,
      "loss": 5.0454,
      "step": 1500
    },
    {
      "epoch": 0.7485029940119761,
      "grad_norm": 5.86322546005249,
      "learning_rate": 4.812874251497006e-05,
      "loss": 4.7139,
      "step": 2000
    },
    {
      "epoch": 0.9356287425149701,
      "grad_norm": 6.5238728523254395,
      "learning_rate": 4.766092814371258e-05,
      "loss": 4.4958,
      "step": 2500
    },
    {
      "epoch": 1.122754491017964,
      "grad_norm": 5.51873779296875,
      "learning_rate": 4.7193113772455094e-05,
      "loss": 4.2781,
      "step": 3000
    },
    {
      "epoch": 1.3098802395209581,
      "grad_norm": 5.937721252441406,
      "learning_rate": 4.672529940119761e-05,
      "loss": 4.132,
      "step": 3500
    },
    {
      "epoch": 1.4970059880239521,
      "grad_norm": 4.8670454025268555,
      "learning_rate": 4.625748502994012e-05,
      "loss": 4.0012,
      "step": 4000
    },
    {
      "epoch": 1.6841317365269461,
      "grad_norm": 7.414938926696777,
      "learning_rate": 4.578967065868264e-05,
      "loss": 3.8871,
      "step": 4500
    },
    {
      "epoch": 1.8712574850299402,
      "grad_norm": 5.887128829956055,
      "learning_rate": 4.532185628742515e-05,
      "loss": 3.7902,
      "step": 5000
    },
    {
      "epoch": 2.058383233532934,
      "grad_norm": 6.00935697555542,
      "learning_rate": 4.485404191616767e-05,
      "loss": 3.7193,
      "step": 5500
    },
    {
      "epoch": 2.245508982035928,
      "grad_norm": 6.415641784667969,
      "learning_rate": 4.438622754491018e-05,
      "loss": 3.6396,
      "step": 6000
    },
    {
      "epoch": 2.432634730538922,
      "grad_norm": 8.159634590148926,
      "learning_rate": 4.3918413173652696e-05,
      "loss": 3.5865,
      "step": 6500
    },
    {
      "epoch": 2.6197604790419162,
      "grad_norm": 6.705329895019531,
      "learning_rate": 4.345059880239521e-05,
      "loss": 3.516,
      "step": 7000
    },
    {
      "epoch": 2.80688622754491,
      "grad_norm": 7.023128509521484,
      "learning_rate": 4.298278443113773e-05,
      "loss": 3.4695,
      "step": 7500
    },
    {
      "epoch": 2.9940119760479043,
      "grad_norm": 6.735691547393799,
      "learning_rate": 4.251497005988024e-05,
      "loss": 3.4153,
      "step": 8000
    },
    {
      "epoch": 3.181137724550898,
      "grad_norm": 6.550085067749023,
      "learning_rate": 4.204715568862276e-05,
      "loss": 3.3358,
      "step": 8500
    },
    {
      "epoch": 3.3682634730538923,
      "grad_norm": 6.80443000793457,
      "learning_rate": 4.157934131736527e-05,
      "loss": 3.2892,
      "step": 9000
    },
    {
      "epoch": 3.555389221556886,
      "grad_norm": 7.148873329162598,
      "learning_rate": 4.111152694610779e-05,
      "loss": 3.2632,
      "step": 9500
    },
    {
      "epoch": 3.7425149700598803,
      "grad_norm": 8.171914100646973,
      "learning_rate": 4.06437125748503e-05,
      "loss": 3.226,
      "step": 10000
    },
    {
      "epoch": 3.929640718562874,
      "grad_norm": 7.29646635055542,
      "learning_rate": 4.0175898203592816e-05,
      "loss": 3.1886,
      "step": 10500
    },
    {
      "epoch": 4.116766467065868,
      "grad_norm": 6.445671558380127,
      "learning_rate": 3.970808383233533e-05,
      "loss": 3.1321,
      "step": 11000
    },
    {
      "epoch": 4.303892215568863,
      "grad_norm": 7.1941680908203125,
      "learning_rate": 3.9240269461077844e-05,
      "loss": 3.0815,
      "step": 11500
    },
    {
      "epoch": 4.491017964071856,
      "grad_norm": 8.414961814880371,
      "learning_rate": 3.877245508982036e-05,
      "loss": 3.0545,
      "step": 12000
    },
    {
      "epoch": 4.67814371257485,
      "grad_norm": 7.821828365325928,
      "learning_rate": 3.830464071856288e-05,
      "loss": 3.0084,
      "step": 12500
    },
    {
      "epoch": 4.865269461077844,
      "grad_norm": 7.124011516571045,
      "learning_rate": 3.783682634730539e-05,
      "loss": 2.9708,
      "step": 13000
    },
    {
      "epoch": 5.052395209580839,
      "grad_norm": 7.18727970123291,
      "learning_rate": 3.736901197604791e-05,
      "loss": 2.9596,
      "step": 13500
    },
    {
      "epoch": 5.2395209580838324,
      "grad_norm": 8.0758056640625,
      "learning_rate": 3.6901197604790425e-05,
      "loss": 2.8954,
      "step": 14000
    },
    {
      "epoch": 5.426646706586826,
      "grad_norm": 7.508564472198486,
      "learning_rate": 3.6433383233532936e-05,
      "loss": 2.8438,
      "step": 14500
    },
    {
      "epoch": 5.61377245508982,
      "grad_norm": 9.506235122680664,
      "learning_rate": 3.596556886227545e-05,
      "loss": 2.8684,
      "step": 15000
    },
    {
      "epoch": 5.800898203592815,
      "grad_norm": 7.7841620445251465,
      "learning_rate": 3.5497754491017964e-05,
      "loss": 2.8213,
      "step": 15500
    },
    {
      "epoch": 5.9880239520958085,
      "grad_norm": 7.363437652587891,
      "learning_rate": 3.502994011976048e-05,
      "loss": 2.8103,
      "step": 16000
    },
    {
      "epoch": 6.175149700598802,
      "grad_norm": 8.8853178024292,
      "learning_rate": 3.456212574850299e-05,
      "loss": 2.7307,
      "step": 16500
    },
    {
      "epoch": 6.362275449101796,
      "grad_norm": 7.093719959259033,
      "learning_rate": 3.409431137724551e-05,
      "loss": 2.7147,
      "step": 17000
    },
    {
      "epoch": 6.549401197604791,
      "grad_norm": 7.116903781890869,
      "learning_rate": 3.362649700598803e-05,
      "loss": 2.6994,
      "step": 17500
    },
    {
      "epoch": 6.736526946107785,
      "grad_norm": 8.903289794921875,
      "learning_rate": 3.3158682634730545e-05,
      "loss": 2.6822,
      "step": 18000
    },
    {
      "epoch": 6.923652694610778,
      "grad_norm": 8.162405014038086,
      "learning_rate": 3.2690868263473056e-05,
      "loss": 2.6411,
      "step": 18500
    },
    {
      "epoch": 7.110778443113772,
      "grad_norm": 8.175800323486328,
      "learning_rate": 3.222305389221557e-05,
      "loss": 2.5721,
      "step": 19000
    },
    {
      "epoch": 7.297904191616767,
      "grad_norm": 8.379719734191895,
      "learning_rate": 3.1755239520958084e-05,
      "loss": 2.5768,
      "step": 19500
    },
    {
      "epoch": 7.485029940119761,
      "grad_norm": 8.917682647705078,
      "learning_rate": 3.12874251497006e-05,
      "loss": 2.5477,
      "step": 20000
    },
    {
      "epoch": 7.672155688622754,
      "grad_norm": 7.361007213592529,
      "learning_rate": 3.081961077844311e-05,
      "loss": 2.5426,
      "step": 20500
    },
    {
      "epoch": 7.859281437125748,
      "grad_norm": 8.715112686157227,
      "learning_rate": 3.035179640718563e-05,
      "loss": 2.5341,
      "step": 21000
    },
    {
      "epoch": 8.046407185628743,
      "grad_norm": 8.057814598083496,
      "learning_rate": 2.9883982035928144e-05,
      "loss": 2.5009,
      "step": 21500
    },
    {
      "epoch": 8.233532934131736,
      "grad_norm": 9.498292922973633,
      "learning_rate": 2.9416167664670658e-05,
      "loss": 2.4456,
      "step": 22000
    },
    {
      "epoch": 8.42065868263473,
      "grad_norm": 9.989152908325195,
      "learning_rate": 2.894835329341318e-05,
      "loss": 2.4298,
      "step": 22500
    },
    {
      "epoch": 8.607784431137725,
      "grad_norm": 8.347661018371582,
      "learning_rate": 2.8480538922155693e-05,
      "loss": 2.4167,
      "step": 23000
    },
    {
      "epoch": 8.794910179640718,
      "grad_norm": 8.192954063415527,
      "learning_rate": 2.8012724550898207e-05,
      "loss": 2.4076,
      "step": 23500
    },
    {
      "epoch": 8.982035928143713,
      "grad_norm": 8.459442138671875,
      "learning_rate": 2.754491017964072e-05,
      "loss": 2.371,
      "step": 24000
    },
    {
      "epoch": 9.169161676646707,
      "grad_norm": 8.329286575317383,
      "learning_rate": 2.7077095808383235e-05,
      "loss": 2.3403,
      "step": 24500
    },
    {
      "epoch": 9.3562874251497,
      "grad_norm": 8.438761711120605,
      "learning_rate": 2.660928143712575e-05,
      "loss": 2.3011,
      "step": 25000
    },
    {
      "epoch": 9.543413173652695,
      "grad_norm": 8.109665870666504,
      "learning_rate": 2.6141467065868264e-05,
      "loss": 2.3033,
      "step": 25500
    },
    {
      "epoch": 9.730538922155688,
      "grad_norm": 9.171026229858398,
      "learning_rate": 2.5673652694610778e-05,
      "loss": 2.2789,
      "step": 26000
    },
    {
      "epoch": 9.917664670658683,
      "grad_norm": 8.533166885375977,
      "learning_rate": 2.5205838323353292e-05,
      "loss": 2.2806,
      "step": 26500
    },
    {
      "epoch": 10.104790419161677,
      "grad_norm": 8.230419158935547,
      "learning_rate": 2.473802395209581e-05,
      "loss": 2.2458,
      "step": 27000
    },
    {
      "epoch": 10.29191616766467,
      "grad_norm": 8.977728843688965,
      "learning_rate": 2.4270209580838324e-05,
      "loss": 2.2013,
      "step": 27500
    },
    {
      "epoch": 10.479041916167665,
      "grad_norm": 8.850464820861816,
      "learning_rate": 2.3802395209580838e-05,
      "loss": 2.199,
      "step": 28000
    },
    {
      "epoch": 10.66616766467066,
      "grad_norm": 8.572809219360352,
      "learning_rate": 2.3334580838323355e-05,
      "loss": 2.168,
      "step": 28500
    },
    {
      "epoch": 10.853293413173652,
      "grad_norm": 8.772602081298828,
      "learning_rate": 2.286676646706587e-05,
      "loss": 2.1715,
      "step": 29000
    },
    {
      "epoch": 11.040419161676647,
      "grad_norm": 8.167067527770996,
      "learning_rate": 2.2398952095808383e-05,
      "loss": 2.1585,
      "step": 29500
    },
    {
      "epoch": 11.22754491017964,
      "grad_norm": 9.439980506896973,
      "learning_rate": 2.1931137724550898e-05,
      "loss": 2.0925,
      "step": 30000
    },
    {
      "epoch": 11.414670658682635,
      "grad_norm": 7.145727634429932,
      "learning_rate": 2.146332335329341e-05,
      "loss": 2.0934,
      "step": 30500
    },
    {
      "epoch": 11.60179640718563,
      "grad_norm": 9.673128128051758,
      "learning_rate": 2.099550898203593e-05,
      "loss": 2.0934,
      "step": 31000
    },
    {
      "epoch": 11.788922155688622,
      "grad_norm": 8.771438598632812,
      "learning_rate": 2.0527694610778443e-05,
      "loss": 2.0761,
      "step": 31500
    },
    {
      "epoch": 11.976047904191617,
      "grad_norm": 9.77835750579834,
      "learning_rate": 2.0059880239520957e-05,
      "loss": 2.0432,
      "step": 32000
    },
    {
      "epoch": 12.16317365269461,
      "grad_norm": 8.099611282348633,
      "learning_rate": 1.9592065868263475e-05,
      "loss": 2.0378,
      "step": 32500
    },
    {
      "epoch": 12.350299401197605,
      "grad_norm": 9.964682579040527,
      "learning_rate": 1.912425149700599e-05,
      "loss": 2.011,
      "step": 33000
    },
    {
      "epoch": 12.5374251497006,
      "grad_norm": 8.574268341064453,
      "learning_rate": 1.8656437125748503e-05,
      "loss": 1.9966,
      "step": 33500
    },
    {
      "epoch": 12.724550898203592,
      "grad_norm": 9.200620651245117,
      "learning_rate": 1.818862275449102e-05,
      "loss": 2.0109,
      "step": 34000
    },
    {
      "epoch": 12.911676646706587,
      "grad_norm": 8.29273509979248,
      "learning_rate": 1.7720808383233535e-05,
      "loss": 1.9909,
      "step": 34500
    },
    {
      "epoch": 13.098802395209582,
      "grad_norm": 10.381110191345215,
      "learning_rate": 1.725299401197605e-05,
      "loss": 1.9523,
      "step": 35000
    },
    {
      "epoch": 13.285928143712574,
      "grad_norm": 7.817104339599609,
      "learning_rate": 1.6785179640718563e-05,
      "loss": 1.9306,
      "step": 35500
    },
    {
      "epoch": 13.47305389221557,
      "grad_norm": 8.964889526367188,
      "learning_rate": 1.631736526946108e-05,
      "loss": 1.9326,
      "step": 36000
    },
    {
      "epoch": 13.660179640718562,
      "grad_norm": 8.791141510009766,
      "learning_rate": 1.5849550898203595e-05,
      "loss": 1.9031,
      "step": 36500
    },
    {
      "epoch": 13.847305389221557,
      "grad_norm": 9.72434139251709,
      "learning_rate": 1.538173652694611e-05,
      "loss": 1.9101,
      "step": 37000
    },
    {
      "epoch": 14.034431137724551,
      "grad_norm": 10.757913589477539,
      "learning_rate": 1.4913922155688623e-05,
      "loss": 1.8771,
      "step": 37500
    },
    {
      "epoch": 14.221556886227544,
      "grad_norm": 8.57110595703125,
      "learning_rate": 1.4446107784431137e-05,
      "loss": 1.8448,
      "step": 38000
    },
    {
      "epoch": 14.408682634730539,
      "grad_norm": 8.810815811157227,
      "learning_rate": 1.3978293413173655e-05,
      "loss": 1.8643,
      "step": 38500
    },
    {
      "epoch": 14.595808383233534,
      "grad_norm": 8.80759048461914,
      "learning_rate": 1.3510479041916169e-05,
      "loss": 1.8394,
      "step": 39000
    },
    {
      "epoch": 14.782934131736527,
      "grad_norm": 10.77424430847168,
      "learning_rate": 1.3042664670658683e-05,
      "loss": 1.8193,
      "step": 39500
    },
    {
      "epoch": 14.970059880239521,
      "grad_norm": 8.89732837677002,
      "learning_rate": 1.2574850299401197e-05,
      "loss": 1.8374,
      "step": 40000
    }
  ],
  "logging_steps": 500,
  "max_steps": 53440,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 20,
  "save_steps": 10000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.3078344316906752e+16,
  "train_batch_size": 64,
  "trial_name": null,
  "trial_params": null
}
