{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 7.485029940119761,
  "eval_steps": 500,
  "global_step": 20000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.18712574850299402,
      "grad_norm": 3.0724408626556396,
      "learning_rate": 4.953218562874252e-05,
      "loss": 6.584,
      "step": 500
    },
    {
      "epoch": 0.37425149700598803,
      "grad_norm": 4.440441608428955,
      "learning_rate": 4.906437125748503e-05,
      "loss": 5.6518,
      "step": 1000
    },
    {
      "epoch": 0.561377245508982,
      "grad_norm": 4.794414520263672,
      "learning_rate": 4.859655688622755e-05,
      "loss": 5.0454,
      "step": 1500
    },
    {
      "epoch": 0.7485029940119761,
      "grad_norm": 5.86322546005249,
      "learning_rate": 4.812874251497006e-05,
      "loss": 4.7139,
      "step": 2000
    },
    {
      "epoch": 0.9356287425149701,
      "grad_norm": 6.5238728523254395,
      "learning_rate": 4.766092814371258e-05,
      "loss": 4.4958,
      "step": 2500
    },
    {
      "epoch": 1.122754491017964,
      "grad_norm": 5.51873779296875,
      "learning_rate": 4.7193113772455094e-05,
      "loss": 4.2781,
      "step": 3000
    },
    {
      "epoch": 1.3098802395209581,
      "grad_norm": 5.937721252441406,
      "learning_rate": 4.672529940119761e-05,
      "loss": 4.132,
      "step": 3500
    },
    {
      "epoch": 1.4970059880239521,
      "grad_norm": 4.8670454025268555,
      "learning_rate": 4.625748502994012e-05,
      "loss": 4.0012,
      "step": 4000
    },
    {
      "epoch": 1.6841317365269461,
      "grad_norm": 7.414938926696777,
      "learning_rate": 4.578967065868264e-05,
      "loss": 3.8871,
      "step": 4500
    },
    {
      "epoch": 1.8712574850299402,
      "grad_norm": 5.887128829956055,
      "learning_rate": 4.532185628742515e-05,
      "loss": 3.7902,
      "step": 5000
    },
    {
      "epoch": 2.058383233532934,
      "grad_norm": 6.00935697555542,
      "learning_rate": 4.485404191616767e-05,
      "loss": 3.7193,
      "step": 5500
    },
    {
      "epoch": 2.245508982035928,
      "grad_norm": 6.415641784667969,
      "learning_rate": 4.438622754491018e-05,
      "loss": 3.6396,
      "step": 6000
    },
    {
      "epoch": 2.432634730538922,
      "grad_norm": 8.159634590148926,
      "learning_rate": 4.3918413173652696e-05,
      "loss": 3.5865,
      "step": 6500
    },
    {
      "epoch": 2.6197604790419162,
      "grad_norm": 6.705329895019531,
      "learning_rate": 4.345059880239521e-05,
      "loss": 3.516,
      "step": 7000
    },
    {
      "epoch": 2.80688622754491,
      "grad_norm": 7.023128509521484,
      "learning_rate": 4.298278443113773e-05,
      "loss": 3.4695,
      "step": 7500
    },
    {
      "epoch": 2.9940119760479043,
      "grad_norm": 6.735691547393799,
      "learning_rate": 4.251497005988024e-05,
      "loss": 3.4153,
      "step": 8000
    },
    {
      "epoch": 3.181137724550898,
      "grad_norm": 6.550085067749023,
      "learning_rate": 4.204715568862276e-05,
      "loss": 3.3358,
      "step": 8500
    },
    {
      "epoch": 3.3682634730538923,
      "grad_norm": 6.80443000793457,
      "learning_rate": 4.157934131736527e-05,
      "loss": 3.2892,
      "step": 9000
    },
    {
      "epoch": 3.555389221556886,
      "grad_norm": 7.148873329162598,
      "learning_rate": 4.111152694610779e-05,
      "loss": 3.2632,
      "step": 9500
    },
    {
      "epoch": 3.7425149700598803,
      "grad_norm": 8.171914100646973,
      "learning_rate": 4.06437125748503e-05,
      "loss": 3.226,
      "step": 10000
    },
    {
      "epoch": 3.929640718562874,
      "grad_norm": 7.29646635055542,
      "learning_rate": 4.0175898203592816e-05,
      "loss": 3.1886,
      "step": 10500
    },
    {
      "epoch": 4.116766467065868,
      "grad_norm": 6.445671558380127,
      "learning_rate": 3.970808383233533e-05,
      "loss": 3.1321,
      "step": 11000
    },
    {
      "epoch": 4.303892215568863,
      "grad_norm": 7.1941680908203125,
      "learning_rate": 3.9240269461077844e-05,
      "loss": 3.0815,
      "step": 11500
    },
    {
      "epoch": 4.491017964071856,
      "grad_norm": 8.414961814880371,
      "learning_rate": 3.877245508982036e-05,
      "loss": 3.0545,
      "step": 12000
    },
    {
      "epoch": 4.67814371257485,
      "grad_norm": 7.821828365325928,
      "learning_rate": 3.830464071856288e-05,
      "loss": 3.0084,
      "step": 12500
    },
    {
      "epoch": 4.865269461077844,
      "grad_norm": 7.124011516571045,
      "learning_rate": 3.783682634730539e-05,
      "loss": 2.9708,
      "step": 13000
    },
    {
      "epoch": 5.052395209580839,
      "grad_norm": 7.18727970123291,
      "learning_rate": 3.736901197604791e-05,
      "loss": 2.9596,
      "step": 13500
    },
    {
      "epoch": 5.2395209580838324,
      "grad_norm": 8.0758056640625,
      "learning_rate": 3.6901197604790425e-05,
      "loss": 2.8954,
      "step": 14000
    },
    {
      "epoch": 5.426646706586826,
      "grad_norm": 7.508564472198486,
      "learning_rate": 3.6433383233532936e-05,
      "loss": 2.8438,
      "step": 14500
    },
    {
      "epoch": 5.61377245508982,
      "grad_norm": 9.506235122680664,
      "learning_rate": 3.596556886227545e-05,
      "loss": 2.8684,
      "step": 15000
    },
    {
      "epoch": 5.800898203592815,
      "grad_norm": 7.7841620445251465,
      "learning_rate": 3.5497754491017964e-05,
      "loss": 2.8213,
      "step": 15500
    },
    {
      "epoch": 5.9880239520958085,
      "grad_norm": 7.363437652587891,
      "learning_rate": 3.502994011976048e-05,
      "loss": 2.8103,
      "step": 16000
    },
    {
      "epoch": 6.175149700598802,
      "grad_norm": 8.8853178024292,
      "learning_rate": 3.456212574850299e-05,
      "loss": 2.7307,
      "step": 16500
    },
    {
      "epoch": 6.362275449101796,
      "grad_norm": 7.093719959259033,
      "learning_rate": 3.409431137724551e-05,
      "loss": 2.7147,
      "step": 17000
    },
    {
      "epoch": 6.549401197604791,
      "grad_norm": 7.116903781890869,
      "learning_rate": 3.362649700598803e-05,
      "loss": 2.6994,
      "step": 17500
    },
    {
      "epoch": 6.736526946107785,
      "grad_norm": 8.903289794921875,
      "learning_rate": 3.3158682634730545e-05,
      "loss": 2.6822,
      "step": 18000
    },
    {
      "epoch": 6.923652694610778,
      "grad_norm": 8.162405014038086,
      "learning_rate": 3.2690868263473056e-05,
      "loss": 2.6411,
      "step": 18500
    },
    {
      "epoch": 7.110778443113772,
      "grad_norm": 8.175800323486328,
      "learning_rate": 3.222305389221557e-05,
      "loss": 2.5721,
      "step": 19000
    },
    {
      "epoch": 7.297904191616767,
      "grad_norm": 8.379719734191895,
      "learning_rate": 3.1755239520958084e-05,
      "loss": 2.5768,
      "step": 19500
    },
    {
      "epoch": 7.485029940119761,
      "grad_norm": 8.917682647705078,
      "learning_rate": 3.12874251497006e-05,
      "loss": 2.5477,
      "step": 20000
    }
  ],
  "logging_steps": 500,
  "max_steps": 53440,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 20,
  "save_steps": 10000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 6539414099362560.0,
  "train_batch_size": 64,
  "trial_name": null,
  "trial_params": null
}
